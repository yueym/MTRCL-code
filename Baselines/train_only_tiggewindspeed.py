import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import xarray as xr
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
import joblib
import json
import os
import time
from torch.optim.lr_scheduler import ReduceLROnPlateau


class CBAM(nn.Module):
    def __init__(self, channels, reduction=16):
        super(CBAM, self).__init__()
        self.channel_attention = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channels, channels // reduction, 1, bias=False),
            nn.ReLU(),
            nn.Conv2d(channels // reduction, channels, 1, bias=False),
            nn.Sigmoid()
        )
        self.spatial_attention = nn.Sequential(
            nn.Conv2d(2, 1, kernel_size=7, padding=3, bias=False),
            nn.Sigmoid()
        )
        self.global_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(channels, channels)
        self.sigmoid = nn.Sigmoid()

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, x):
        channel_att = self.channel_attention(x)
        x = x * channel_att
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        spatial_att = self.spatial_attention(torch.cat([avg_out, max_out], dim=1))
        x = x * spatial_att
        global_feat = self.global_pool(x)
        global_feat = self.fc(global_feat.view(x.size(0), -1))
        global_att = self.sigmoid(global_feat).view(x.size(0), -1, 1, 1)
        x = x + x * global_att
        return x


class ResNetCBAM_WindOnly(nn.Module):
    def __init__(self, in_channels=1, dropout_rate=0.25):
        super(ResNetCBAM_WindOnly, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, 56, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(56)
        self.relu = nn.ReLU(inplace=True)
        self.shortcut = nn.Conv2d(in_channels, 56, kernel_size=1)
        self.conv2 = nn.Conv2d(56, 56, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(56)
        self.conv3 = nn.Conv2d(56, 56, kernel_size=3, padding=1)
        self.bn3 = nn.BatchNorm2d(56)
        self.conv4 = nn.Conv2d(56, 56, kernel_size=3, padding=1)
        self.bn4 = nn.BatchNorm2d(56)
        self.conv5 = nn.Conv2d(56, 56, kernel_size=3, padding=1)
        self.bn5 = nn.BatchNorm2d(56)
        self.cbam = CBAM(56, reduction=16)
        self.dropout = nn.Dropout(p=dropout_rate)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x, time_emb):
        identity = self.shortcut(x)
        out = self.conv1(x)
        out = self.bn1(out) + time_emb
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        out = out + identity
        out = self.relu(out)
        identity = out
        out = self.conv3(out)
        out = self.bn3(out)
        out = out + identity
        out = self.relu(out)
        identity = out
        out = self.conv4(out)
        out = self.bn4(out)
        out = out + identity
        out = self.relu(out)
        identity = out
        out = self.conv5(out)
        out = self.bn5(out)
        out = out + identity
        out = self.relu(out)
        out = self.dropout(out)
        out = self.cbam(out)
        return out


class ODEFunc_WindOnly(nn.Module):
    def __init__(self, hidden_dim=216, input_dim=1, dropout_rate=0.25):
        super(ODEFunc_WindOnly, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(hidden_dim + input_dim + 5, 512),
            nn.ReLU(),
            nn.Dropout(p=dropout_rate),
            nn.Linear(512, 384),
            nn.ReLU(),
            nn.Dropout(p=dropout_rate),
            nn.Linear(384, 256),
            nn.ReLU(),
            nn.Dropout(p=dropout_rate),
            nn.Linear(256, hidden_dim)
        )
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, h, x, time_features):
        input = torch.cat([h, x, time_features], dim=-1)
        return self.net(input)


class LTC_WindOnly(nn.Module):
    def __init__(self, input_dim=1, hidden_dim=216, output_dim=216, seq_len=4, dt=6.0, dropout_rate=0.25):
        super(LTC_WindOnly, self).__init__()
        self.hidden_dim = hidden_dim
        self.seq_len = seq_len
        self.dt = dt
        self.ode_func = ODEFunc_WindOnly(hidden_dim, input_dim, dropout_rate)
        self.output_layer = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(p=dropout_rate),
            nn.Linear(hidden_dim // 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(p=dropout_rate),
            nn.Linear(hidden_dim, output_dim)
        )
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, x_seq, time_features_seq):
        b, t, c, H, W = x_seq.shape
        x_seq = x_seq.permute(0, 3, 4, 1, 2).reshape(b * H * W, self.seq_len, -1)
        time_features_seq = time_features_seq.unsqueeze(1).unsqueeze(2).repeat(1, H, W, 1, 1).reshape(b * H * W,
                                                                                                      self.seq_len, 5)
        h = torch.zeros(b * H * W, self.hidden_dim).to(x_seq.device)
        dt = self.dt * 0.1
        for k in range(self.seq_len):
            x_k = x_seq[:, k, :]
            t_k = time_features_seq[:, k, :]
            if torch.isnan(h).any():
                h = torch.where(torch.isnan(h), torch.zeros_like(h), h)
            k1 = self.ode_func(h, x_k, t_k)
            k2 = self.ode_func(h + 0.5 * dt * k1, x_k, t_k)
            k3 = self.ode_func(h + 0.5 * dt * k2, x_k, t_k)
            k4 = self.ode_func(h + dt * k3, x_k, t_k)
            k1 = torch.clamp(k1, -10, 10)
            k2 = torch.clamp(k2, -10, 10)
            k3 = torch.clamp(k3, -10, 10)
            k4 = torch.clamp(k4, -10, 10)
            h = h + (dt / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)
            h = torch.clamp(h, -100, 100)
        out = self.output_layer(h)
        out = out.reshape(b, H, W, -1).permute(0, 3, 1, 2)
        return out


class GatedFusion(nn.Module):
    def __init__(self, C1, C2):
        super(GatedFusion, self).__init__()
        self.gate = nn.Sequential(
            nn.Conv2d(C1 + C2, (C1 + C2) // 2, kernel_size=1),
            nn.ReLU(),
            nn.Conv2d((C1 + C2) // 2, C1 + C2, kernel_size=1),
            nn.Sigmoid()
        )
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.xavier_uniform_(m.weight, gain=0.01)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

    def forward(self, resnet_out, ltc_out):
        fused = torch.cat([resnet_out, ltc_out], dim=1)
        gate = self.gate(fused)
        output = gate * fused
        return output


class MLP(nn.Module):
    def __init__(self, input_dim, dropout_rate=0.25):
        super(MLP, self).__init__()
        self.net = nn.Sequential(
            nn.Conv2d(input_dim, 512, kernel_size=1),
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.Dropout(p=dropout_rate),
            nn.Conv2d(512, 384, kernel_size=1),
            nn.BatchNorm2d(384),
            nn.ReLU(),
            nn.Dropout(p=dropout_rate),
            nn.Conv2d(384, 192, kernel_size=1),
            nn.BatchNorm2d(192),
            nn.ReLU(),
            nn.Dropout(p=dropout_rate),
            nn.Conv2d(192, 1, kernel_size=1),
            nn.Sigmoid()
        )
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        return self.net(x)


class WindSpeedPredictor_WindOnly(nn.Module):
    def __init__(self, H, W, dropout_rate=0.25, ltc_hidden_dim=216, cbam_reduction=16):
        super(WindSpeedPredictor_WindOnly, self).__init__()
        self.H = H
        self.W = W
        self.resnet = ResNetCBAM_WindOnly(in_channels=1, dropout_rate=dropout_rate)
        self.ltc = LTC_WindOnly(input_dim=1, hidden_dim=ltc_hidden_dim, output_dim=ltc_hidden_dim,
                                dropout_rate=dropout_rate)
        self.gated_fusion = GatedFusion(56, ltc_hidden_dim)
        self.mlp = MLP(56 + ltc_hidden_dim, dropout_rate=dropout_rate)
        self.time_embed = nn.Sequential(
            nn.Linear(5, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 56)
        )
        for m in self.time_embed:
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
        total_params = sum(p.numel() for p in self.parameters())
        print(f"Total parameters (Wind Only): {total_params:,}")

    def forward(self, wind_spatial, wind_seq, time_features_t, time_features_seq):
        b = wind_spatial.size(0)
        time_emb = self.time_embed(time_features_t).view(b, 56, 1, 1)
        resnet_out = self.resnet(wind_spatial, time_emb)
        ltc_out = self.ltc(wind_seq, time_features_seq)
        fused = self.gated_fusion(resnet_out, ltc_out)
        pred = self.mlp(fused)
        return pred.squeeze(1)


class WindDataset_WindOnly(Dataset):
    def __init__(self, ds_path, H=48, W=96, seq_len=4):
        self.H = H
        self.W = W
        self.seq_len = seq_len
        self.ds = xr.open_dataset(ds_path, cache=False)

        # ç›´æ¥æœç´¢å¹¶æå–tigge_wind_speedå‚æ•°
        self.wind_speed_index = self._find_wind_speed_parameter()

        time_data = self.ds['time_features'].values
        self.time_scaler = StandardScaler()
        normalized_time = self.time_scaler.fit_transform(time_data)
        self.ds['time_features_normalized'] = xr.DataArray(
            normalized_time,
            dims=self.ds['time_features'].dims,
            coords={'sample': self.ds['time_features'].coords['sample']}
        )
        times = pd.to_datetime({
            'year': time_data[:, 0],
            'month': time_data[:, 1],
            'day': time_data[:, 2],
            'hour': time_data[:, 3]
        })
        self.ds = self.ds.assign_coords(time=("sample", times)).sortby('time')
        self.time_points = np.unique(self.ds.time.values)
        self.T = len(self.time_points)
        self.samples_per_time = H * W
        self.sample_indices = np.arange(self.T - self.seq_len + 1)

    def _find_wind_speed_parameter(self):
        """ç›´æ¥åœ¨æ•°æ®é›†ä¸­æœç´¢tigge_wind_speedå‚æ•°"""
        print("\n" + "=" * 60)
        print("ğŸ” åœ¨æ•°æ®é›†ä¸­æœç´¢tigge_wind_speedå‚æ•°")
        print("=" * 60)

        # æ£€æŸ¥æ•°æ®é›†ä¸­æ˜¯å¦ç›´æ¥æœ‰tigge_wind_speedå˜é‡
        if 'tigge_wind_speed' in self.ds.data_vars:
            print("âœ… æ‰¾åˆ°ç›´æ¥çš„tigge_wind_speedå˜é‡ï¼")
            wind_speed_data = self.ds['tigge_wind_speed'].values
            print(f"tigge_wind_speedæ•°æ®å½¢çŠ¶: {wind_speed_data.shape}")
            print(f"æ•°å€¼èŒƒå›´: [{wind_speed_data.min():.6f}, {wind_speed_data.max():.6f}]")
            print(f"å¹³å‡å€¼: {wind_speed_data.mean():.6f}")
            print("ä½¿ç”¨ç›´æ¥çš„tigge_wind_speedå˜é‡")
            return 'direct'  # æ ‡è®°ä¸ºç›´æ¥ä½¿ç”¨

        # å¦‚æœæ²¡æœ‰ç›´æ¥å˜é‡ï¼Œæ£€æŸ¥æ•°æ®é›†çš„å±æ€§å’Œåæ ‡
        print("æ•°æ®é›†å˜é‡åˆ—è¡¨:")
        for var in self.ds.data_vars:
            print(f"  - {var}: {self.ds[var].shape}")

        print("\næ•°æ®é›†åæ ‡:")
        for coord in self.ds.coords:
            print(
                f"  - {coord}: {self.ds.coords[coord].shape if hasattr(self.ds.coords[coord], 'shape') else 'scalar'}")

        # æ£€æŸ¥X_tiggeçš„ç»´åº¦ä¿¡æ¯
        if 'X_tigge' in self.ds.data_vars:
            print(f"\nX_tiggeå˜é‡ä¿¡æ¯:")
            print(f"  å½¢çŠ¶: {self.ds['X_tigge'].shape}")
            print(f"  ç»´åº¦: {self.ds['X_tigge'].dims}")

            # æ£€æŸ¥æ˜¯å¦æœ‰ç‰¹å¾åç§°ç»´åº¦
            if len(self.ds['X_tigge'].dims) > 1:
                feature_dim = self.ds['X_tigge'].dims[-1]  # é€šå¸¸ç‰¹å¾ç»´åº¦æ˜¯æœ€åä¸€ä¸ª
                print(f"  ç‰¹å¾ç»´åº¦åç§°: {feature_dim}")

                # æ£€æŸ¥ç‰¹å¾ç»´åº¦çš„åæ ‡
                if feature_dim in self.ds.coords:
                    feature_names = self.ds.coords[feature_dim].values
                    print(f"  ç‰¹å¾åç§°åˆ—è¡¨:")
                    for i, name in enumerate(feature_names):
                        print(f"    {i:2d}: {name}")

                    # æœç´¢tigge_wind_speed
                    if 'tigge_wind_speed' in feature_names:
                        wind_index = list(feature_names).index('tigge_wind_speed')
                        print(f"\nğŸ¯ æ‰¾åˆ°tigge_wind_speedï¼ç´¢å¼•ä½ç½®: {wind_index}")

                        # éªŒè¯æ•°æ®
                        tigge_data = self.ds['X_tigge'].values
                        wind_speed_data = tigge_data[:, wind_index]
                        print(f"éªŒè¯æ•°æ®:")
                        print(f"  æ•°æ®å½¢çŠ¶: {wind_speed_data.shape}")
                        print(f"  æ•°å€¼èŒƒå›´: [{wind_speed_data.min():.6f}, {wind_speed_data.max():.6f}]")
                        print(f"  å¹³å‡å€¼: {wind_speed_data.mean():.6f}")
                        print(f"  æ ‡å‡†å·®: {wind_speed_data.std():.6f}")

                        # åˆç†æ€§æ£€æŸ¥
                        if wind_speed_data.min() >= 0 and 0 < wind_speed_data.mean() < 20:
                            print("âœ… æ•°æ®åˆç†æ€§æ£€æŸ¥é€šè¿‡")
                        else:
                            print("âš ï¸  æ•°æ®å¯èƒ½å­˜åœ¨å¼‚å¸¸ï¼Œä½†ç»§ç»­ä½¿ç”¨")

                        return wind_index
                    else:
                        print("âŒ åœ¨ç‰¹å¾åç§°ä¸­æœªæ‰¾åˆ°tigge_wind_speed")
                        print("å¯ç”¨çš„ç‰¹å¾åç§°:", list(feature_names))
                else:
                    print(f"âš ï¸  ç‰¹å¾ç»´åº¦{feature_dim}æ²¡æœ‰åæ ‡ä¿¡æ¯")
            else:
                print("âš ï¸  X_tiggeåªæœ‰ä¸€ä¸ªç»´åº¦")

        # å¦‚æœéƒ½æ²¡æ‰¾åˆ°ï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„æœç´¢æ–¹å¼
        print("\nğŸ” å°è¯•å…¶ä»–æœç´¢æ–¹å¼...")

        # æ£€æŸ¥æ˜¯å¦æœ‰åŒ…å«wind_speedçš„å˜é‡å
        wind_related_vars = [var for var in self.ds.data_vars if 'wind' in var.lower()]
        if wind_related_vars:
            print(f"æ‰¾åˆ°åŒ…å«'wind'çš„å˜é‡: {wind_related_vars}")
            for var in wind_related_vars:
                if 'speed' in var.lower():
                    print(f"âœ… å¯èƒ½çš„é£é€Ÿå˜é‡: {var}")
                    return var

        # æœ€åçš„fallbackï¼šå¦‚æœå®åœ¨æ‰¾ä¸åˆ°ï¼Œè¯¢é—®ç”¨æˆ·
        print("\nâŒ æ— æ³•è‡ªåŠ¨æ‰¾åˆ°tigge_wind_speedå‚æ•°")
        print("è¯·æ£€æŸ¥æ•°æ®é›†ç»“æ„ï¼Œæˆ–æ‰‹åŠ¨æŒ‡å®šå‚æ•°ä½ç½®")

        # è¿”å›ä¸€ä¸ªé»˜è®¤å€¼ï¼Œä½†ä¼šåœ¨åç»­å¤„ç†ä¸­æŠ¥é”™
        raise ValueError("æœªæ‰¾åˆ°tigge_wind_speedå‚æ•°ï¼è¯·æ£€æŸ¥æ•°æ®é›†ç»“æ„ã€‚")

    def _extract_wind_speed_data(self, sample_mask):
        """æ ¹æ®æ‰¾åˆ°çš„ç´¢å¼•æå–wind_speedæ•°æ®"""
        if self.wind_speed_index == 'direct':
            # ç›´æ¥ä½¿ç”¨tigge_wind_speedå˜é‡
            return self.ds['tigge_wind_speed'].sel(sample=sample_mask).values.reshape(self.H, self.W, 1)
        elif isinstance(self.wind_speed_index, str):
            # ä½¿ç”¨æ‰¾åˆ°çš„å˜é‡å
            return self.ds[self.wind_speed_index].sel(sample=sample_mask).values.reshape(self.H, self.W, 1)
        elif isinstance(self.wind_speed_index, int):
            # ä½¿ç”¨ç´¢å¼•ä½ç½®
            tigge_full = self.ds['X_tigge'].sel(sample=sample_mask).values.reshape(self.H, self.W, -1)
            return tigge_full[:, :, self.wind_speed_index:self.wind_speed_index + 1]
        else:
            raise ValueError(f"æ— æ•ˆçš„wind_speed_index: {self.wind_speed_index}")

    def __len__(self):
        return len(self.sample_indices)

    def __getitem__(self, idx):
        actual_idx = self.sample_indices[idx]
        t = actual_idx + self.seq_len - 1
        seq_times = self.time_points[t - self.seq_len + 1: t + 1]
        seq_data = []
        time_features_seq = []

        for time in seq_times:
            mask = self.ds.time == time
            # ä½¿ç”¨æ–°çš„æå–æ–¹æ³•
            wind_data = self._extract_wind_speed_data(mask)
            seq_data.append(wind_data)
            time_features = self.ds['time_features_normalized'].sel(sample=mask).values[0]
            time_features_seq.append(time_features)

        wind_seq = np.stack(seq_data)  # shape: (seq_len, H, W, 1)
        time_features_seq = np.stack(time_features_seq)

        time_t = self.time_points[t]
        mask_t = self.ds.time == time_t
        wind_spatial = self._extract_wind_speed_data(mask_t)
        target = self.ds['y'].sel(sample=mask_t).values.reshape(self.H, self.W)
        time_features_t = time_features_seq[-1]

        return {
            'wind_spatial': torch.from_numpy(wind_spatial).float().permute(2, 0, 1),  # (1, H, W)
            'wind_seq': torch.from_numpy(wind_seq).float().permute(0, 3, 1, 2),  # (seq_len, 1, H, W)
            'time_features_t': torch.from_numpy(time_features_t).float(),
            'time_features_seq': torch.from_numpy(time_features_seq).float(),
            'target': torch.from_numpy(target).float()
        }


def calculate_metrics_with_mape(pred, target):
    """è®¡ç®—åŒ…å«MAPEçš„8é¡¹æŒ‡æ ‡"""
    pred = pred.flatten()
    target = target.flatten()

    FA = ((pred - target).abs() < 1).float().mean().item() * 100
    RMSE = torch.sqrt(torch.mean((pred - target) ** 2)).item()
    MAE = torch.mean((pred - target).abs()).item()
    mean_target = torch.mean(target).item()
    rRMSE = (RMSE / mean_target) * 100 if mean_target > 0 else 0
    rMAE = (MAE / mean_target) * 100 if mean_target > 0 else 0
    R = torch.corrcoef(torch.stack([pred, target]))[0, 1].item()

    ss_tot = torch.sum((target - mean_target) ** 2).item()
    ss_res = torch.sum((target - pred) ** 2).item()
    R2 = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0

    mask = target > 1e-6
    if torch.sum(mask) > 0:
        MAPE = torch.mean(torch.abs((target[mask] - pred[mask]) / target[mask])).item() * 100
    else:
        MAPE = 0.0

    return {
        'FA': FA, 'RMSE': RMSE, 'MAE': MAE, 'rRMSE': rRMSE,
        'rMAE': rMAE, 'R': R, 'R2': R2, 'MAPE': MAPE
    }


def train_model(model, train_loader, val_loader, device, num_epochs=80):
    """è®­ç»ƒæ¨¡å‹"""
    print(f"å¼€å§‹è®­ç»ƒæ¨¡å‹ (åªä½¿ç”¨tigge_wind_speedå†å²æ•°æ®)...")

    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs('checkpoints_wind', exist_ok=True)

    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    criterion = nn.SmoothL1Loss()
    optimizer = optim.AdamW(model.parameters(), lr=3.9e-4, weight_decay=1e-4)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)

    # è®­ç»ƒè®°å½•
    train_losses = []
    val_losses = []
    best_val_loss = float('inf')
    patience_counter = 0
    patience = 15

    # åŠ è½½æ ‡å‡†åŒ–å™¨
    scaler_target = joblib.load('./show_relevance_visualization/target_scaler.pkl')
    target_data_min = scaler_target.data_min_[0]
    target_range = 1 / scaler_target.scale_[0]

    for epoch in range(num_epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_batches = 0

        for batch_idx, batch in enumerate(train_loader):
            try:
                wind_spatial = batch['wind_spatial'].to(device)
                wind_seq = batch['wind_seq'].to(device)
                time_features_t = batch['time_features_t'].to(device)
                time_features_seq = batch['time_features_seq'].to(device)
                target = batch['target'].to(device)

                optimizer.zero_grad()
                output = model(wind_spatial, wind_seq, time_features_t, time_features_seq)
                loss = criterion(output, target)
                loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

                optimizer.step()

                train_loss += loss.item()
                train_batches += 1

                if (batch_idx + 1) % 50 == 0:
                    print(f'Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx + 1}/{len(train_loader)}], '
                          f'Loss: {loss.item():.6f}')

            except Exception as e:
                print(f"è®­ç»ƒæ‰¹æ¬¡é”™è¯¯ {batch_idx}: {str(e)}")
                continue

        avg_train_loss = train_loss / train_batches if train_batches > 0 else float('inf')
        train_losses.append(avg_train_loss)

        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_batches = 0
        all_preds = []
        all_targets = []

        with torch.no_grad():
            for batch_idx, batch in enumerate(val_loader):
                try:
                    wind_spatial = batch['wind_spatial'].to(device)
                    wind_seq = batch['wind_seq'].to(device)
                    time_features_t = batch['time_features_t'].to(device)
                    time_features_seq = batch['time_features_seq'].to(device)
                    target = batch['target'].to(device)

                    output = model(wind_spatial, wind_seq, time_features_t, time_features_seq)
                    loss = criterion(output, target)

                    val_loss += loss.item()
                    val_batches += 1

                    all_preds.append(output.cpu())
                    all_targets.append(target.cpu())

                except Exception as e:
                    print(f"éªŒè¯æ‰¹æ¬¡é”™è¯¯ {batch_idx}: {str(e)}")
                    continue

        avg_val_loss = val_loss / val_batches if val_batches > 0 else float('inf')
        val_losses.append(avg_val_loss)

        # è®¡ç®—éªŒè¯é›†æŒ‡æ ‡
        if len(all_preds) > 0:
            all_preds = torch.cat(all_preds, dim=0)
            all_targets = torch.cat(all_targets, dim=0)

            # åæ ‡å‡†åŒ–
            all_preds_orig = (all_preds.numpy() * target_range) + target_data_min
            all_targets_orig = (all_targets.numpy() * target_range) + target_data_min
            all_preds_orig = np.clip(all_preds_orig, 0, 100)
            all_targets_orig = np.clip(all_targets_orig, 0, 100)

            val_metrics = calculate_metrics_with_mape(
                torch.from_numpy(all_preds_orig),
                torch.from_numpy(all_targets_orig)
            )

            print(f'Epoch [{epoch + 1}/{num_epochs}]:')
            print(f'  Train Loss: {avg_train_loss:.6f}')
            print(f'  Val Loss: {avg_val_loss:.6f}')
            print(f'  Val FA: {val_metrics["FA"]:.2f}%, RMSE: {val_metrics["RMSE"]:.4f}, '
                  f'MAE: {val_metrics["MAE"]:.4f}, R: {val_metrics["R"]:.4f}')
            print('-' * 60)

            # å­¦ä¹ ç‡è°ƒæ•´
        scheduler.step(avg_val_loss)

        # æ—©åœå’Œæ¨¡å‹ä¿å­˜
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            patience_counter = 0
            torch.save(model.state_dict(), 'checkpoints_wind/best_model_wind_only.pth')
            print(f'ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ŒéªŒè¯æŸå¤±: {best_val_loss:.6f}')
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print(f'æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch + 1} è½®åœæ­¢è®­ç»ƒ')
            break

        # æ¯5è½®ä¿å­˜ä¸€æ¬¡æ¨¡å‹
        if (epoch + 1) % 5 == 0:
            torch.save(model.state_dict(), f'checkpoints_wind/model_epoch_{epoch + 1}_wind_only.pth')

        # ä¿å­˜è®­ç»ƒå†å²
    training_history = {
        'train_losses': train_losses,
        'val_losses': val_losses,
        'best_val_loss': best_val_loss
    }

    with open('checkpoints_wind/training_history_wind_only.json', 'w') as f:
        json.dump(training_history, f, indent=2)

    print("è®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.6f}")
    print("æ¨¡å‹å·²ä¿å­˜è‡³ checkpoints_wind/best_model_wind_only.pth")

    return model


def test_model(model, test_loader, device):
    """æµ‹è¯•æ¨¡å‹"""
    model.eval()
    all_preds = []
    all_targets = []

    scaler_target = joblib.load('./show_relevance_visualization/target_scaler.pkl')
    target_data_min = scaler_target.data_min_[0]
    target_range = 1 / scaler_target.scale_[0]

    print("å¼€å§‹æµ‹è¯•æ¨¡å‹...")
    with torch.no_grad():
        for batch_idx, batch in enumerate(test_loader):
            try:
                wind_spatial = batch['wind_spatial'].to(device)
                wind_seq = batch['wind_seq'].to(device)
                time_features_t = batch['time_features_t'].to(device)
                time_features_seq = batch['time_features_seq'].to(device)
                target = batch['target'].to(device)

                output = model(wind_spatial, wind_seq, time_features_t, time_features_seq)

                all_preds.append(output.cpu())
                all_targets.append(target.cpu())

                if (batch_idx + 1) % 50 == 0:
                    print(f"å·²å¤„ç†æµ‹è¯•æ‰¹æ¬¡: {batch_idx + 1}/{len(test_loader)}")

            except Exception as e:
                print(f"æµ‹è¯•æ‰¹æ¬¡é”™è¯¯ {batch_idx}: {str(e)}")
                continue

    all_preds = torch.cat(all_preds, dim=0).numpy()
    all_targets = torch.cat(all_targets, dim=0).numpy()

    # åæ ‡å‡†åŒ–
    all_preds_orig = (all_preds * target_range) + target_data_min
    all_targets_orig = (all_targets * target_range) + target_data_min
    all_preds_orig = np.clip(all_preds_orig, 0, 100)
    all_targets_orig = np.clip(all_targets_orig, 0, 100)

    # è®¡ç®—æµ‹è¯•æŒ‡æ ‡
    test_metrics = calculate_metrics_with_mape(
        torch.from_numpy(all_preds_orig),
        torch.from_numpy(all_targets_orig)
    )

    print("\n" + "=" * 60)
    print("MTRCLæ¨¡å‹æµ‹è¯•ç»“æœ - ä»…ä½¿ç”¨tigge_wind_speedå†å²æ•°æ®")
    print("=" * 60)
    print(f"å‡†ç¡®ç‡ (FA):           {test_metrics['FA']:.2f}%")
    print(f"å‡æ–¹æ ¹è¯¯å·® (RMSE):     {test_metrics['RMSE']:.4f} m/s")
    print(f"å¹³å‡ç»å¯¹è¯¯å·® (MAE):    {test_metrics['MAE']:.4f} m/s")
    print(f"ç›¸å¯¹RMSE (rRMSE):     {test_metrics['rRMSE']:.2f}%")
    print(f"ç›¸å¯¹MAE (rMAE):       {test_metrics['rMAE']:.2f}%")
    print(f"ç›¸å…³ç³»æ•° (R):         {test_metrics['R']:.4f}")
    print(f"å†³å®šç³»æ•° (RÂ²):        {test_metrics['R2']:.4f}")
    print(f"å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·® (MAPE): {test_metrics['MAPE']:.2f}%")
    print("=" * 60)

    # ä¿å­˜æµ‹è¯•ç»“æœ
    with open('checkpoints_wind/test_results_wind_only.json', 'w') as f:
        json.dump({k: float(v) for k, v in test_metrics.items()}, f, indent=2)

    return test_metrics


# ä¸»ç¨‹åº
if __name__ == "__main__":
    # Windowså¤šè¿›ç¨‹ä¿æŠ¤
    import multiprocessing

    multiprocessing.freeze_support()

    H, W = 48, 96
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # é¦–å…ˆæ£€æŸ¥ä¸€ä¸ªæ•°æ®é›†æ–‡ä»¶çš„ç»“æ„
    print("ğŸ” æ£€æŸ¥æ•°æ®é›†ç»“æ„...")
    try:
        sample_ds = xr.open_dataset("./show_relevance_visualization/train.nc", cache=False)
        print("âœ… æˆåŠŸæ‰“å¼€æ•°æ®é›†ï¼Œå¼€å§‹æœç´¢tigge_wind_speedå‚æ•°...")
        sample_ds.close()
    except Exception as e:
        print(f"âŒ æ— æ³•æ‰“å¼€æ•°æ®é›†: {e}")
        exit(1)

    # åŠ è½½æ•°æ®é›†ï¼ˆä¼šè‡ªåŠ¨æœç´¢tigge_wind_speedï¼‰
    print("åŠ è½½æ•°æ®é›†...")
    batch_size = 16

    try:
        print("ğŸ” æœç´¢è®­ç»ƒé›†ä¸­çš„tigge_wind_speed...")
        train_ds = WindDataset_WindOnly("./show_relevance_visualization/train.nc", H, W)

        print("ğŸ” æœç´¢éªŒè¯é›†ä¸­çš„tigge_wind_speed...")
        val_ds = WindDataset_WindOnly("./show_relevance_visualization/val.nc", H, W)

        print("ğŸ” æœç´¢æµ‹è¯•é›†ä¸­çš„tigge_wind_speed...")
        test_ds = WindDataset_WindOnly("./show_relevance_visualization/test.nc", H, W)

    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥æ•°æ®é›†ä¸­æ˜¯å¦åŒ…å«tigge_wind_speedå‚æ•°")
        exit(1)

    # æ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)
    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)
    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=False)

    print("âœ… æ•°æ®é›†åŠ è½½å®Œæˆ")
    print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_ds)}")
    print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(val_ds)}")
    print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_ds)}")

    # éªŒè¯æ•°æ®æå–æ˜¯å¦æ­£ç¡®
    print("\nğŸ§ª éªŒè¯æ•°æ®æå–...")
    sample_batch = train_ds[0]
    print(f"å•ä¸ªæ ·æœ¬æ•°æ®å½¢çŠ¶éªŒè¯:")
    print(f"  wind_spatial shape: {sample_batch['wind_spatial'].shape} (æœŸæœ›: [1, 48, 96])")
    print(f"  wind_seq shape: {sample_batch['wind_seq'].shape} (æœŸæœ›: [4, 1, 48, 96])")
    print(f"  target shape: {sample_batch['target'].shape} (æœŸæœ›: [48, 96])")
    print(
        f"  wind_spatialæ•°å€¼èŒƒå›´: [{sample_batch['wind_spatial'].min():.4f}, {sample_batch['wind_spatial'].max():.4f}]")
    print("âœ… æ•°æ®æå–éªŒè¯å®Œæˆï¼\n")

    # åˆå§‹åŒ–æ¨¡å‹
    print("åˆå§‹åŒ–æ¨¡å‹...")
    model = WindSpeedPredictor_WindOnly(
        H, W,
        dropout_rate=0.25,
        ltc_hidden_dim=216,
        cbam_reduction=16
    ).to(device)

    # è®­ç»ƒæ¨¡å‹
    print("å¼€å§‹è®­ç»ƒ...")
    start_time = time.time()
    model = train_model(model, train_loader, val_loader, device, num_epochs=1)
    end_time = time.time()
    print(f"è®­ç»ƒè€—æ—¶: {(end_time - start_time) / 3600:.2f} å°æ—¶")

    # æµ‹è¯•æ¨¡å‹
    print("å¼€å§‹æµ‹è¯•...")
    model.load_state_dict(torch.load('checkpoints_wind/best_model_wind_only.pth', map_location=device))
    test_metrics = test_model(model, test_loader, device)

    print("\nğŸ‰ è®­ç»ƒå’Œæµ‹è¯•å®Œæˆï¼")
    print(f"æ¨¡å‹æ–‡ä»¶ä¿å­˜åœ¨: checkpoints_wind/best_model_wind_only.pth")
    print(f"æµ‹è¯•ç»“æœä¿å­˜åœ¨: checkpoints_wind/test_results_wind_only.json")

    # è¾“å‡ºæœ€ç»ˆç»“æœæ‘˜è¦
    print(f"\nğŸ“Š æœ€ç»ˆæµ‹è¯•ç»“æœæ‘˜è¦:")
    print(f"FA: {test_metrics['FA']:.2f}%")
    print(f"RMSE: {test_metrics['RMSE']:.4f} m/s")
    print(f"MAE: {test_metrics['MAE']:.4f} m/s")
    print(f"R: {test_metrics['R']:.4f}")
    print(f"MAPE: {test_metrics['MAPE']:.2f}%")